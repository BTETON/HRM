matrix[n2,n1] ystar;
matrix[n2,n1] linpred;
//matrix[n1,K]  mLambdaK;
//compute linear predictions
for(j in 1:n2){
for(i in 1:n1){
linpred[j,i] <- beta0+x1[i]'*betaI+x2[j]'*betaT;
}
}
// assign estimates for missing RE values to ystar
ystar <- y-linpred;
for(i in 1:nMiss) {
ystar[x2Miss[i],x1Miss[i]] <- yMissRE[i];
}
//transform factor assignment probabilities to matrix
//for(i in 1:n1) {
//  mLambdaK[i] <- to_row_vector(lambdaK[i]);
//}
}
model {
//declare intermediary variables
matrix[n1,n1] Sigma1; //kernel matrices
matrix[n2,n2] Sigma2;
matrix[n1,n1] Q1; //eigenvectors from eigendecomposition
matrix[n2,n2] Q2;
vector[n1] R1; // eigenvalues from eigendecomposition
vector[n2] R2;
matrix[n2,n1] eigenvalues;
//compute kernel matrices
Sigma1 <- var1*exp(xd1*bw1);
for(i in 1:n1) {
Sigma1[i,i] <- Sigma1[i,i]+0.00001;
}
Sigma2 <- exp(xd2*bw2);
for(i in 1:n2) {
Sigma2[i,i] <- Sigma2[i,i]+0.00001;
}
//compute eigendecompositions
Q1 <- eigenvectors_sym(Sigma1);
Q2 <- eigenvectors_sym(Sigma2);
R1 <- eigenvalues_sym(Sigma1);
R2 <- eigenvalues_sym(Sigma2);
eigenvalues <- calculate_eigenvalues(R2,R1,n2,n1,sigma1);
//priors for parameters
bw1 ~ cauchy(0,2.5);
bw2 ~ cauchy(0,2.5);
var1 ~ lognormal(0,1);
sigma1 ~ lognormal(0,1);
beta0 ~ cauchy(0,10);
for(i in 1:m1) {
betaI[i] ~ cauchy(0,2.5);
}
for(i in 1:m2) {
betaT[i] ~ cauchy(0,2.5);
}
//for(i in 1:n1) {
//  to_vector(lambdaK[i]) ~ dirichlet(alpha);
//}
increment_log_prob(
-0.5*sum(ystar .* kron_mvprod(Q1,Q2, // calculates -0.5 * y’ (K1 ox K2) y
kron_mvprod(transpose(Q1),transpose(Q2),ystar)./eigenvalues))
-0.5*sum(log(eigenvalues)) // calculates logdet(K1 ox K2)
);
}
"
#Create data input for STAN
model.dat <- list(m1=dim(dfX)[2]-1, m2=dim(dfMonth)[2]-2,
n1=dim(dfX)[1]  , n2=dim(dfMonth)[1],
nMiss=dim(indxMiss)[1], x1Miss=indxMiss[,1], x2Miss=indxMiss[,2],
x1 = as.matrix(dfX[,c(2:6)]),
x2 = as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)]),
y  = t(as.matrix(dfY[,c(2:dim(dfY)[2])])),
xd1 = calcGramMtx(as.matrix(dfX[,c(2:6)])),
xd2 = calcGramMtx(as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)])),
control=list(adapt_delta=0.9,max_treedepth=12))
# run model
rstan_options(auto_write = TRUE)
options(mc.cores = 4)
stan.fit <- stan(model_code=GPLP.mdl.code,
model_name="Gaussian_Process",
data=model.dat,
iter=500,warmup=100,
chains=4,thin=1,seed=101)
dfRsp <- rbind(dfF2F[dfF2F$ADM1_NAME==adm,cols],dfmVAM[dfmVAM$ADM1_NAME==adm,cols])
plot(density(dfRsp$RFCS))
plot(density(dfRsp$FCS))
plot(density(log(dfRsp$FCS)))
plot(density(dfRsp$FCS))
plot(density(log(dfRsp$FCS)))
adm <- 'Abyan'
dfRsp <- rbind(dfF2F[dfF2F$ADM1_NAME==adm,cols],dfmVAM[dfmVAM$ADM1_NAME==adm,cols])
dfRsp$FCS <- log(dfRsp$FCS)
#dfRsp$FCS <- (dfRsp$FCS-mean(dfRsp$FCS))/sd(dfRsp$FCS )
dfRsp$HHSize <- (dfRsp$HHSize-mean(dfRsp$HHSize))/sd(dfRsp$HHSize )
dfY <- dcast(data = dfRsp[,c(1,2,3)],formula = RspID~tMonth,fun.aggregate = sum,value.var = "FCS")
dfX <- dfRsp[match(dfY$RspID,dfRsp$RspID),c('RspID','HHSize','ActiveMobileNmbr','HoHSex','FoodAssistance_YN','IDP_YN')]
indxMiss <- which(as.matrix(dfY[,c(2:14)])==0, arr.ind=TRUE)
# row_vector[n1] x1[m1]; //time invariant attributes by respondent
# row_vector[n2] x2[m2]; //time variant attributes by time step
# matrix[n2,n1] Y;   //observations for respondent at each time step
#Create data input for STAN
model.dat <- list(m1=dim(dfX)[2]-1, m2=dim(dfMonth)[2]-2,
n1=dim(dfX)[1]  , n2=dim(dfMonth)[1],
nMiss=dim(indxMiss)[1], x1Miss=indxMiss[,1], x2Miss=indxMiss[,2],
x1 = as.matrix(dfX[,c(2:6)]),
x2 = as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)]),
y  = t(as.matrix(dfY[,c(2:dim(dfY)[2])])),
xd1 = calcGramMtx(as.matrix(dfX[,c(2:6)])),
xd2 = calcGramMtx(as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)])),
control=list(adapt_delta=0.9,max_treedepth=12))
# run model
rstan_options(auto_write = TRUE)
options(mc.cores = 4)
stan.fit <- stan(model_code=GPLP.mdl.code,
model_name="Gaussian_Process",
data=model.dat,
iter=500,warmup=100,
chains=4,thin=1,seed=101)
#Yemen Seasonal Calendar
dfMonth <- unique(dfmVAM[,c('tMonth','SvyDate')])
dfMonth$monthNum <- as.numeric(format(dfMonth$SvyDate, "%m"))
dfMonth$lndPrep <- 0
dfMonth$lndPrep[is.element(dfMonth$monthNum,c(7,8,3,4))] <- 1
dfMonth$harvest <- 0
dfMonth$harvest[is.element(dfMonth$monthNum,c(7,10,11,12,6))] <- 1
dfMonth$rainSsn <- 0
dfMonth$rainSsn[is.element(dfMonth$monthNum,c(7,8,9,3,4,5))] <- 1
dfMonth$leanSsn <- 0
dfMonth$leanSsn[is.element(dfMonth$monthNum,c(4,5))] <- 1
dfMonth$milkSsn <- 0
dfMonth$milkSsn[is.element(dfMonth$monthNum,c(7,8,9,10,4,5,6))] <- 1
dfMonth$peakH20 <- 0
dfMonth$peakH20[is.element(dfMonth$monthNum,c(1,2))] <- 1
dfMonth$peakLbr <- 0
dfMonth$peakLbr[is.element(dfMonth$monthNum,c(9,10,11,12,1))] <- 1
adm <- 'Abyan'
dfRsp <- rbind(dfF2F[dfF2F$ADM1_NAME==adm,cols],dfmVAM[dfmVAM$ADM1_NAME==adm,cols])
dfRsp$FCS <- log(dfRsp$FCS)
#dfRsp$FCS <- (dfRsp$FCS-mean(dfRsp$FCS))/sd(dfRsp$FCS )
dfRsp$HHSize <- (dfRsp$HHSize-mean(dfRsp$HHSize))/sd(dfRsp$HHSize )
dfY <- dcast(data = dfRsp[,c(1,2,3)],formula = RspID~tMonth,fun.aggregate = sum,value.var = "FCS")
dfX <- dfRsp[match(dfY$RspID,dfRsp$RspID),c('RspID','HHSize','ActiveMobileNmbr','HoHSex','FoodAssistance_YN','IDP_YN')]
indxMiss <- which(as.matrix(dfY[,c(2:14)])==0, arr.ind=TRUE)
# row_vector[n1] x1[m1]; //time invariant attributes by respondent
# row_vector[n2] x2[m2]; //time variant attributes by time step
# matrix[n2,n1] Y;   //observations for respondent at each time step
#Create data input for STAN
model.dat <- list(m1=dim(dfX)[2]-1, m2=dim(dfMonth)[2]-2,
n1=dim(dfX)[1]  , n2=dim(dfMonth)[1],
nMiss=dim(indxMiss)[1], x1Miss=indxMiss[,1], x2Miss=indxMiss[,2],
x1 = as.matrix(dfX[,c(2:6)]),
x2 = as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)]),
y  = t(as.matrix(dfY[,c(2:dim(dfY)[2])])),
xd1 = calcGramMtx(as.matrix(dfX[,c(2:6)])),
xd2 = calcGramMtx(as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)])),
control=list(adapt_delta=0.9,max_treedepth=12))
# run model
rstan_options(auto_write = TRUE)
options(mc.cores = 4)
stan.fit <- stan(model_code=GPLP.mdl.code,
model_name="Gaussian_Process",
data=model.dat,
iter=500,warmup=100,
chains=4,thin=1,seed=101)
stan.fit <- stan(model_code=GPLP.mdl.code,
model_name="Gaussian_Process",
data=model.dat,
iter=500,warmup=100,
chains=1,thin=1,seed=101)
extract(stan.fit)
stan.fit@stanmodel@model_code
stan.fit@inits
stan.fit@sim
GPLP.mdl.code <- "
functions {
// return (A ox B) v where:
// A is n1 x n1, B = n2 x n2, V = n2 x n1 = reshape(v,n2,n1)
matrix kron_mvprod(matrix A, matrix B, matrix V) {
return transpose(A * transpose(B * V));
}
// A is a length n1 vector, B is a length n2 vector.
// Treating them as diagonal matrices, this calculates:
// v = (A ox B + sigma2)ˆ{-1}
// and returns the n1 x n2 matrix V = reshape(v,n1,n2)
matrix calculate_eigenvalues(vector A, vector B, int n1, int n2, real sigma2) {
matrix[n1,n2] e;
for(i in 1:n1) {
for(j in 1:n2) {
e[i,j] <- (A[i] * B[j] + sigma2);
}
}
return(e);
}
}
data {
int<lower=1> m1; //number of time invariant components of design matrix (respondent attributes)
int<lower=1> m2; //number of time variant componets of design matrix
int<lower=1> n1; //number of unique respondents
int<lower=1> n2; //number of time steps
int<lower=0> nMiss;         //number of missing observations
int<lower=0> x1Miss[nMiss]; //indices for missing respondents
int<lower=0> x2Miss[nMiss]; //indices for missing time steps
vector[m1] x1[n1]; //time invariant attributes by respondent
vector[m2] x2[n2]; //time variant attributes by time step
matrix[n2,n1]  y;      //observations for respondent at each time step
matrix[n1,n1] xd1; //distance calculation for kernel of x1
matrix[n2,n2] xd2; //distance calculation for kernel of x2
//int<lower=0> K; //number of desired factors
}
transformed data {
//int m; //total width of design matrix
//vector[n1] X[m]; //design matrix
//vector[K]   alpha; //probability of belonging to class
//matrix[n1,n1] xd1; //distance calculation for kernel of x1
//matrix[n2,n2] xd2; //distance calculation for kernel of x2
//compute design matrix
//m <- m1+m2;
//for(i in 1:n1) {
//  X[i] <- append_row(x0[i],x1[i]);
//}
//intialize alpha
//for(i in 1:K){
//  alpha[i] <- 0.1;
//}
//compute||x1-x1`||= <x-x`>.*<x-x`>
//for(i in 1:n1) {
//  xd1[i,i] <- 0;
//  for(j in (i+1):n1) {
//    xd1[i,j] <- -dot_self(x1[i]-x1[j]);
//    xd1[j,i] <- xd1[i,j];
//  }
//}
//compute||x2-x2`||= <x2-x2`>.*<x2-x2`>
//for(i in 1:n2) {
//  xd2[i,i] <- 0;
//  for(j in (i+1):n2) {
//    xd1[i,j] <- -dot_self(x2[i]-x2[j]);
//    xd1[j,i] <- xd1[i,j];
//  }
//}
}
parameters {
real<lower=0> var1; // signal variance
real<lower=0>  bw1; // bandwidth 1
real<lower=0>  bw2; // bandwidth 2
real<lower=0.00001> sigma1; //jitter
real<lower=0.00001> sigma2; //jitter
//real          beta0; //intercept
//vector[m1]    betaI; //estimated coefficients for time invariant attributes
//vector[m2]    betaT; //estimated coefficients for time variant attributes
vector[nMiss] yMissRE; //innovations for missing values
//simplex[K] lambdaK[n1]; //assigned probability of factor to each respondent
}
transformed parameters {
matrix[n2,n1] ystar;
//matrix[n2,n1] linpred;
//matrix[n1,K]  mLambdaK;
//compute linear predictions
//for(j in 1:n2){
//  for(i in 1:n1){
//    linpred[j,i] <- beta0+x1[i]'*betaI+x2[j]'*betaT;
//  }
//}
// assign estimates for missing RE values to ystar
ystar <- y;//-linpred;
for(i in 1:nMiss) {
ystar[x2Miss[i],x1Miss[i]] <- yMissRE[i];
}
//transform factor assignment probabilities to matrix
//for(i in 1:n1) {
//  mLambdaK[i] <- to_row_vector(lambdaK[i]);
//}
}
model {
//declare intermediary variables
matrix[n1,n1] Sigma1; //kernel matrices
matrix[n2,n2] Sigma2;
matrix[n1,n1] Q1; //eigenvectors from eigendecomposition
matrix[n2,n2] Q2;
vector[n1] R1; // eigenvalues from eigendecomposition
vector[n2] R2;
matrix[n2,n1] eigenvalues;
//compute kernel matrices
Sigma1 <- var1*exp(xd1*bw1);
for(i in 1:n1) {
Sigma1[i,i] <- Sigma1[i,i]+0.00001;
}
Sigma2 <- exp(xd2*bw2);
for(i in 1:n2) {
Sigma2[i,i] <- Sigma2[i,i]+0.00001;
}
//compute eigendecompositions
Q1 <- eigenvectors_sym(Sigma1);
Q2 <- eigenvectors_sym(Sigma2);
R1 <- eigenvalues_sym(Sigma1);
R2 <- eigenvalues_sym(Sigma2);
eigenvalues <- calculate_eigenvalues(R2,R1,n2,n1,sigma1);
//priors for parameters
bw1 ~ cauchy(0,2.5);
bw2 ~ cauchy(0,2.5);
var1 ~ lognormal(0,1);
sigma1 ~ lognormal(0,1);
//beta0 ~ cauchy(0,10);
//for(i in 1:m1) {
//  betaI[i] ~ cauchy(0,2.5);
//}
//for(i in 1:m2) {
//  betaT[i] ~ cauchy(0,2.5);
//}
//for(i in 1:n1) {
//  to_vector(lambdaK[i]) ~ dirichlet(alpha);
//}
increment_log_prob(
-0.5*sum(ystar .* kron_mvprod(Q1,Q2, // calculates -0.5 * y’ (K1 ox K2) y
kron_mvprod(transpose(Q1),transpose(Q2),ystar)./eigenvalues))
-0.5*sum(log(eigenvalues)) // calculates logdet(K1 ox K2)
);
}
"
#Create data input for STAN
model.dat <- list(m1=dim(dfX)[2]-1, m2=dim(dfMonth)[2]-2,
n1=dim(dfX)[1]  , n2=dim(dfMonth)[1],
nMiss=dim(indxMiss)[1], x1Miss=indxMiss[,1], x2Miss=indxMiss[,2],
x1 = as.matrix(dfX[,c(2:6)]),
x2 = as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)]),
y  = t(as.matrix(dfY[,c(2:dim(dfY)[2])])),
xd1 = calcGramMtx(as.matrix(dfX[,c(2:6)])),
xd2 = calcGramMtx(as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)])),
control=list(adapt_delta=0.9,max_treedepth=12))
# run model
rstan_options(auto_write = TRUE)
options(mc.cores = 4)
stan.fit <- stan(model_code=GPLP.mdl.code,
model_name="Gaussian_Process",
data=model.dat,
iter=500,warmup=100,
chains=1,thin=1,seed=101)
xd1 = calcGramMtx(as.matrix(dfX[,c(2:6)]))
which(xd1=inf,arr.ind = TRUE)
which(xd1==inf,arr.ind = TRUE)
which(xd1==Inf,arr.ind = TRUE)
which(xd1==NA,arr.ind = TRUE)
sumary(xd1)
summary(xd1)
max(xd1)
min(xd1)
?eigen
eigen(xd1)
x <- eigen(xd1)
x$values
which(xd1==0,arr.ind = TRUE)
xd1 <- xd1+0.00001
diag(xd1) <- 0
View(xd1)
xd2 <- xd2+0.00001
calcGramMtx <- function(mtx){
n <- dim(mtx)[1]
f <- function(x,y) (x-y)%*%(x-y)
B <- outer(1:n,1:n,Vectorize(function(i,j) f(mtx[i,],mtx[j,])))+0.00001
diag(B) <- 0
return(B)
}
#Create data input for STAN
model.dat <- list(m1=dim(dfX)[2]-1, m2=dim(dfMonth)[2]-2,
n1=dim(dfX)[1]  , n2=dim(dfMonth)[1],
nMiss=dim(indxMiss)[1], x1Miss=indxMiss[,1], x2Miss=indxMiss[,2],
x1 = as.matrix(dfX[,c(2:6)]),
x2 = as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)]),
y  = t(as.matrix(dfY[,c(2:dim(dfY)[2])])),
xd1 = calcGramMtx(as.matrix(dfX[,c(2:6)])),
xd2 = calcGramMtx(as.matrix(dfMonth[,c(1,4,5,6,7,8,9,10)])),
control=list(adapt_delta=0.9,max_treedepth=12))
# run model
rstan_options(auto_write = TRUE)
options(mc.cores = 4)
stan.fit <- stan(model_code=GPLP.mdl.code,
model_name="Gaussian_Process",
data=model.dat,
iter=500,warmup=100,
chains=1,thin=1,seed=101)
xd1[1,13]
install_github(“validmeasures/spatialsampler”)
install_github('validmeasures/spatialsampler)
install_github('validmeasures/spatialsampler')
library(devtools)
install.packages('devtools')
rep(1,10)
?svd
dfHRM <- read.csv('/media/pato/DATA/Dev/features_config_id_78.csv')
dfSng <- read.csv('/media/pato/DATA/Dev/y_gps_config_78.csv')
SVD <- svd(dfHRM[,c(1:512)],nu=512,nv=512)
head(SVD$d)
SVD$d
?scale
eof.fit <- function(X, p) {
s <- svd(X)
s$u[, 1:p] %*% diag(s$d[1:p]) %*% t(s$v[, 1:p])
}
eof <- function(X, p) {
svd(X)$u[, 1:p]
}
eof.summary <- function(X, per = c(25, 50, 95), col = c(1:3)) {
s <- svd(X)
svd.var <- s$d^2/sum(s$d^2) * 100
svd.cum.var <- cumsum(s$d^2/sum(s$d^2)) * 100
svd.per <- (1:ncol(X))[sapply(per, function(p) {min(which(svd.cum.var >= p))})]
svd.col <- cumsum(s$d[col]^2/sum(s$d^2)) * 100
list(var = svd.var, cum.var = svd.cum.var, per.exp = svd.per,
per = per, col.exp = svd.col, col = col)
}
lvis <- as.matrix(dfHRM[,c(1:512)])
X <- scale(lvis[, 3:ncol(lvis)], scale = FALSE)
eof.var <- eof.summary(X, per = c(50, 90, 95, 99), col = c(1:512))
eof.var
par(mfrow = c(1, 2), mar = c(4, 6, 4, 2))
plot(eof.var[[1]], ylim = c(0, 100), cex.lab = 2, xlab = "SVD column",
ylab = "Percent variance explained", pch = 19, cex = 0.2)
abline(v = eof.var[[3]], col = rainbow(length(eof.var[[3]])))
legend("topright", lty = 1, legend = paste("Column ", eof.var[[3]],
": ", eof.var[[4]], "%", sep = ""), col = rainbow(length(eof.var[[3]])),bty = "n")
plot(eof.var[[2]], ylim = c(0, 100), cex.lab = 2, xlab = "SVD column",ylab = "Cumulative percent variance explained", pch = 19, cex = 0.2)
abline(v = eof.var[[3]], col = rainbow(length(eof.var[[3]])))
legend("bottomright", lty = 1, legend = paste("Column ", eof.var[[3]],
": ", eof.var[[4]], "%", sep = ""), col = rainbow(length(eof.var[[3]])),bty = "n")
eof(lvis,180)
rm(SVD)
###Choose appropriate number of dimensions
nVar <- 180
dfPCA <- cbind(dfHRM[,c('i','j')],eof(lvis,180))
dfHRM <- merge(dfSng,dfPCA,on=c('i','j'))[,c(3:(dim(dfPCA)[2]+3)
iDups <- which(duplicated(dfHRM[,c('lat')]))
dfHRM$lat[iDups] <- jitter(dfHRM$lat[iDups])
iDups <- which(duplicated(dfHRM[,c('lon')]))
dfHRM$lon[iDups] <- jitter(dfHRM$lon[iDups])
dfHRM <- merge(dfSng,dfPCA,on=c('i','j'))[,c(3:(dim(dfPCA)[2]+3)]
dfHRM <- merge(dfSng,dfPCA,on=c('i','j'))[,c(3:(dim(dfPCA)[2]+3)]
dfHRM <- merge(dfSng,dfPCA,on=c('i','j'))
dfHRM <- merge(dfSng,dfPCA,on=c('i','j'))[,c(3:(dim(dfPCA)[2]+3))]
dfPCA <- cbind(dfHRM[,c('i','j')],eof(lvis,180))
dfHRM <- merge(dfSng,dfPCA,on=c('i','j'))[,c(3:(dim(dfPCA)[2]+3))]
iDups <- which(duplicated(dfHRM[,c('lat')]))
dfHRM$lat[iDups] <- jitter(dfHRM$lat[iDups])
iDups <- which(duplicated(dfHRM[,c('lon')]))
dfHRM$lon[iDups] <- jitter(dfHRM$lon[iDups])
fmla <- as.formula(paste('y~',paste(colnames(dfHRM)[c(4:253)],sep='',collapse='+'),sep=''))
library(spBayes)
library(spatial)
library(glmnet)
library(caret)
folds <- createFolds(dfHRM$y,k=5)
i <-  folds$Fold1
X <- cbind(rep(1,dim(dfHRM)[1]),as.matrix(dfHRM[-i,c(4:(dim(dfHRM)[2]))]))
ridge.mdl  <- cv.glmnet(x=X, y=dfHRM$y[-i], alpha=0, lambda=10^seq(3,-2,by=-.1))
lambda     <- ridge.mdl$lambda.min
ridge.mdl  <- glmnet(x=X, y=dfHRM$y[-i], alpha=0, lambda=lambda)
beta.prior <- unclass(coefficients(ridge.mdl))@x
rsd.vario  <- dfHRM$y[-i]-predict(ridge.mdl,newx=X)
dim(X)
head(X)
dim(dfHRM)
X <- cbind(rep(1,dim(dfHRM[-i,])[1]),as.matrix(dfHRM[-i,c(4:(dim(dfHRM)[2]))]))
head(X)
dim(X)
x[,181]
X[,181]
X[,1]
ridge.mdl  <- cv.glmnet(x=X, y=dfHRM$y[-i], alpha=0, lambda=10^seq(3,-2,by=-.1))
lambda     <- ridge.mdl$lambda.min
ridge.mdl  <- glmnet(x=X, y=dfHRM$y[-i], alpha=0, lambda=lambda)
beta.prior <- unclass(coefficients(ridge.mdl))@x
rsd.vario  <- dfHRM$y[-i]-predict(ridge.mdl,newx=X)
plot(density(rsd.vario))
ggplot(cbind(dfHRM[-i,c('lat','lon')],rsd=rsd.vario), aes(x = lat, y = lon, color=s0))+geom_point()
?aes
?geom_point
?scale_colour_gradient
ggplot(cbind(dfHRM[-i,c('lat','lon')],rsd=rsd.vario), aes(x = lat, y = lon, color=s0))+geom_point()+scale_fill_gradient2()
ggplot(cbind(dfHRM[-i,c('lat','lon')],rsd=rsd.vario), aes(x = lat, y = lon, color=s0))+geom_point()+scale_colour_gradient2()
beta.prior
starting <- list(#"beta"    = beta.prior,
"phi"      = 3/10,
"sigma.sq" = 2,
"tau.sq"   = 2)
tuning <- list("phi"=0.5,
"sigma.sq"=0.5, "tau.sq"=0.5)
priors <- list(#beta.Norm = list(rep(0,length(beta.prior)), diag(1000,length(beta.prior))),
phi.Unif = c(0.1,10),
sigma.sq.IG = c(2, 2),
tau.sq.IG = c(2, 2))
sp.mdl.exp <- spLM(s0~1,data=as.data.frame(rsd.vario),coords=as.matrix(dfHRM[-i,c('lat','lon')]),cov.model='exponential',
starting = starting,tuning = tuning, priors = priors,n.samples=5000)
?spRecover
mdl.exp.rslt <- spRecover(sp.mdl.exp,start=1001)
savehistory("/media/pato/DATA/Dev/HRMBayesSng.Rhistory")
mdl.exp.rslt
summary(mdl.exp.rslt)
starting <- list(#"beta"    = beta.prior,
"nu"       = 2,
"phi"      = 3/10,
"sigma.sq" = 2,
"tau.sq"   = 2)
tuning <- list("phi"=0.5, "nu"=0.5,
"sigma.sq"=0.5, "tau.sq"=0.5)
priors <- list(#beta.Norm = list(rep(0,length(beta.prior)), diag(1000,length(beta.prior))),
nu.Unif  = c(0.5,5),
phi.Unif = c(0.1,10),
sigma.sq.IG = c(2, 2),
tau.sq.IG = c(2, 2))
sp.mdl.mtn <- spLM(s0~1,data=as.data.frame(rsd.vario),coords=as.matrix(dfHRM[-i,c('lat','lon')]),cov.model='matern',
starting = starting,tuning = tuning, priors = priors,n.samples=5000)
mdl.mtn.rslt <- spRecover(sp.mdl.exp,start=1001)
savehistory("/media/pato/DATA/Dev/HRMBayesSng.Rhistory")
